{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find NounPhrases using the Spacy library\n",
    "Spacy is considered to be a very good library for POS analysis\n",
    "This script is to find out the noun phrases (potential n-grams) using the spacy api. The nounphrases found will be added to the main dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the instance of the spacy\n",
    "from spacy.en import English\n",
    "nlp = English()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the cases from whihc we want to find the noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname=\"all_jim_case_large.txt\"\n",
    "with open(fname, 'r') as myfile:\n",
    "    data=myfile.read()\n",
    "data=data.split('-------BREAK--------')\n",
    "cases=[case.strip() for case in data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the stop word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwordsFile = open('stopwordsss.txt', 'r')\n",
    "stopwords=stopwordsFile.read()\n",
    "stopwordList=stopwords.split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility function to print a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printDict(dictToPrint):\n",
    "    for key,value in dictToPrint.items():\n",
    "        print(key)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to cleanup the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanupText(str):\n",
    "    str = str.strip();\n",
    "    str = re.sub('/[^A-Za-z0-9 _\\-\\+\\&\\,\\#]/', '', str)\n",
    "    str = str.replace('\"', ' ')\n",
    "    str = str.replace('\\\"', ' ')\n",
    "    str = str.replace(')', ' ')\n",
    "    str = str.replace('(', ' ')\n",
    "    str = str.replace('>', ' ')\n",
    "    str = str.replace('@', ' ')\n",
    "    str = str.replace('<', ' ')\n",
    "    str = str.replace(':', ' ')\n",
    "    str = str.replace('.', ' ')\n",
    "    str = str.replace('[', ' ')\n",
    "    str = str.replace(']', ' ')\n",
    "    str = str.replace('_', ' ')\n",
    "    str = str.replace(',', ' ')\n",
    "    str = str.replace('#', ' ')\n",
    "    str = str.replace('-', ' ')\n",
    "    str = str.replace('/', ' ')\n",
    "    str = str.replace('\"', ' ')\n",
    "    str = str.replace('\\n', ' ')\n",
    "    str = str.replace('~', ' ')\n",
    "    str = re.sub(r'\\d+', ' ', str)\n",
    "    word_sent = [word for word in str.lower().split(\" \") if word not in stopwordList and len(word)>1]\n",
    "    if (len(word_sent) > 2):\n",
    "        finalSent = ' '.join(word_sent)\n",
    "    else:\n",
    "        finalSent=''\n",
    "\n",
    "    return(finalSent.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the cases using the spacy object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [nlp(d) for d in cases]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the noun phrases from the parsed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arrNounPhrase=[]\n",
    "for idx, doc in enumerate(docs):\n",
    "    testDoc=doc\n",
    "    nounPhrases = [np.text for np in testDoc.noun_chunks]\n",
    "    tokenizedNounPhrases=[]\n",
    "    for tt in nounPhrases:\n",
    "        finalSent=cleanupText(tt)\n",
    "        if(len(finalSent)>0):\n",
    "            tokenizedNounPhrases.append(finalSent)\n",
    "    arrNounPhrase.append(tokenizedNounPhrases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arrNounPhrase is a list of list. For the further analysis we want it to be flat list so that we can do frequency count of the keys. One way to flatten the list is defining a lamba function\n",
    "this has been taken from http://stackoverflow.com/questions/952914/making-a-flat-list-out-of-list-of-lists-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "flatNounPhraseList=flatten(arrNounPhrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with key as the ngram and value as the number of times it has appeared in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5139\n"
     ]
    }
   ],
   "source": [
    "nounPhraseDictCount={}\n",
    "for tt in flatNounPhraseList:\n",
    "    finalSent=tt\n",
    "    if (nounPhraseDictCount.get(finalSent)):\n",
    "        nounPhraseDictCount[finalSent] += 1\n",
    "    else:\n",
    "        nounPhraseDictCount[finalSent] = 1\n",
    "print(len(nounPhraseDictCount))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally we want to pick up only those keys (ngrams) that have appeared at least 10 times in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefilter fuel pressure 42\n",
      "duct burner spread 27\n",
      "bearing metal temp 62\n",
      "thrust bearing temperature 19\n",
      "generator bearing temperature 14\n",
      "brg metal temp 17\n",
      "pressure steam tag 15\n",
      "combustion monitor spread 11\n",
      "fw pmp mtr outboard vib 10\n",
      "attemperator spray flow 15\n",
      "lube oil pressure 89\n",
      "bearing metal temperature 42\n",
      "fuel gas temperature 15\n",
      "siemens tower drive metal temperature 13\n",
      "ip drum conductivity 11\n",
      "gear bearing temp 13\n",
      "water injection flow 20\n",
      "oil drain temperature 11\n",
      "compressor outlet temp range 10\n",
      "exhaust gas port 12\n",
      "fuel gas pressure 18\n",
      "ip drum differential 25\n",
      "water jacket inlet pressure 15\n",
      "drive generator bearing temperature 11\n",
      "fuel gas temp 21\n",
      "bearing metal temps 21\n",
      "ib bearing temp 16\n",
      "power turbine positions 14\n",
      "driver thrust bearing 10\n",
      "intercooler outlet temperature 14\n",
      "gen slipring bearing vib 11\n",
      "sh dsh outlet temperature 10\n",
      "generator bearing temperature nde 47\n",
      "exhaust duct pressure 10\n",
      "lp steam flow 33\n",
      "nde seal outlet flow 15\n",
      "rear gear bearing temp 12\n",
      "motor stator temps 11\n",
      "hrsg drum spec cndc sample 19\n",
      "thrust bearing position 12\n",
      "generator de bearing temperature 13\n",
      "lube oil filter 21\n",
      "turbine temperature wheelspace 16\n",
      "gearbox oil temperature 12\n",
      "generator bearing temp 14\n",
      "intercooler coolant pressure 13\n",
      "drive generator bearing temp 11\n",
      "ob bearing temp 17\n",
      "oil drain temp 13\n",
      "hydrogen gas pressure 11\n",
      "pump ib brg temp 12\n",
      "exhaust steam temperature 11\n",
      "thrust bearing temp 28\n",
      "ip hrsg drum sample 19\n",
      "generator bearing metal temp 13\n",
      "motor winding temp 12\n",
      "thrust bearing axial position 21\n",
      "motor stator temperature 12\n",
      "compressor inlet pressure 40\n",
      "lube oil temperature 71\n",
      "ip blowdown silica 22\n",
      "ip fw flow 11\n",
      "fuel gas flow 24\n",
      "stator coil vib 10\n",
      "generator bearing temperature de 15\n",
      "de bearing temperature 18\n",
      "fw pmp axial probe 12\n",
      "gen bearing temp 53\n",
      "seal gas filter 15\n",
      "nde bearing temperature 24\n",
      "de seal outlet flow 19\n",
      "motor winding temperature 13\n",
      "motor ob bearing temp 12\n",
      "turbine bearing vibrations 20\n",
      "exhaust temp spread 11\n",
      "rear strut pressure 39\n",
      "lube oil filter dp 19\n",
      "gt vibration transducer 11\n",
      "turbine outlet temp 12\n",
      "cylinder exhaust temperature 14\n",
      "inlet filter dp 10\n",
      "agb chip detector 10\n",
      "seal oil pressure 13\n",
      "gearbox bearing temp 17\n",
      "brg mtl temp 17\n",
      "bearing vibration sensor 11\n",
      "gearbox oil temp 20\n",
      "lublind cooler temperature 12\n",
      "de seal dp 16\n",
      "bearing oil drain temperature 10\n",
      "rear gen bearing temp 49\n",
      "duct burner fuel gas flow 16\n",
      "de bearing vibrations 12\n",
      "axial position sensor 15\n",
      "motor stator temp 24\n",
      "rotor vib brg 12\n",
      "turbine exhaust pressure 16\n",
      "lube oil temp 40\n",
      "thermal mechanical tag 12\n",
      "generator bearing temp\n",
      "duct burner fuel gas flow\n",
      "generator bearing metal temp\n",
      "ip hrsg drum sample\n",
      "stator coil vib\n",
      "motor winding temp\n",
      "motor stator temps\n",
      "pressure steam tag\n",
      "fuel gas temperature\n",
      "lube oil temperature\n",
      "turbine temperature wheelspace\n",
      "ip drum conductivity\n",
      "bearing metal temp\n",
      "power turbine positions\n",
      "fuel gas temp\n",
      "bearing vibration sensor\n",
      "lube oil filter dp\n",
      "ip blowdown silica\n",
      "ip drum differential\n",
      "de bearing vibrations\n",
      "generator bearing temperature nde\n",
      "lp steam flow\n",
      "inlet filter dp\n",
      "gearbox oil temperature\n",
      "gear bearing temp\n",
      "water injection flow\n",
      "thrust bearing temperature\n",
      "combustion monitor spread\n",
      "generator bearing temperature\n",
      "exhaust duct pressure\n",
      "intercooler outlet temperature\n",
      "thrust bearing temp\n",
      "hrsg drum spec cndc sample\n",
      "thrust bearing position\n",
      "prefilter fuel pressure\n",
      "ob bearing temp\n",
      "exhaust temp spread\n",
      "exhaust steam temperature\n",
      "gt vibration transducer\n",
      "nde seal outlet flow\n",
      "drive generator bearing temp\n",
      "de seal outlet flow\n",
      "duct burner spread\n",
      "oil drain temperature\n",
      "pump ib brg temp\n",
      "fuel gas flow\n",
      "turbine outlet temp\n",
      "oil drain temp\n",
      "gen slipring bearing vib\n",
      "de seal dp\n",
      "rear gear bearing temp\n",
      "gearbox oil temp\n",
      "nde bearing temperature\n",
      "exhaust gas port\n",
      "compressor outlet temp range\n",
      "axial position sensor\n",
      "thrust bearing axial position\n",
      "driver thrust bearing\n",
      "seal gas filter\n",
      "brg metal temp\n",
      "rotor vib brg\n",
      "lube oil filter\n",
      "agb chip detector\n",
      "drive generator bearing temperature\n",
      "cylinder exhaust temperature\n",
      "seal oil pressure\n",
      "generator de bearing temperature\n",
      "sh dsh outlet temperature\n",
      "gearbox bearing temp\n",
      "water jacket inlet pressure\n",
      "bearing metal temps\n",
      "ip fw flow\n",
      "generator bearing temperature de\n",
      "brg mtl temp\n",
      "motor winding temperature\n",
      "thermal mechanical tag\n",
      "siemens tower drive metal temperature\n",
      "lube oil pressure\n",
      "motor ob bearing temp\n",
      "motor stator temp\n",
      "fw pmp mtr outboard vib\n",
      "de bearing temperature\n",
      "rear gen bearing temp\n",
      "turbine exhaust pressure\n",
      "fw pmp axial probe\n",
      "gen bearing temp\n",
      "lube oil temp\n",
      "bearing oil drain temperature\n",
      "turbine bearing vibrations\n",
      "compressor inlet pressure\n",
      "lublind cooler temperature\n",
      "ib bearing temp\n",
      "fuel gas pressure\n",
      "intercooler coolant pressure\n",
      "motor stator temperature\n",
      "bearing metal temperature\n",
      "attemperator spray flow\n",
      "rear strut pressure\n",
      "hydrogen gas pressure\n"
     ]
    }
   ],
   "source": [
    "finalPOSList={}\n",
    "for dd in nounPhraseDictCount:\n",
    "    ddArr=dd.split(\" \")\n",
    "    if(len(ddArr)>1):\n",
    "        #atelaset 10 occurence of the phrase\n",
    "        if(nounPhraseDictCount[dd]>9):\n",
    "            print(dd,nounPhraseDictCount[dd])\n",
    "            finalPOSList[dd]=nounPhraseDictCount[dd]\n",
    "        \n",
    "printDict(finalPOSList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfoming statistical analysis of the nounphrases found\n",
    "We want to find out what is the min and max number of occurence of each noun phrase. Which is the most occuring nounphrase \n",
    "\n",
    "To do this I have used numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 .... 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#create a new array that only has the value of the ngrams\n",
    "statArr=[]\n",
    "for key,value in finalPOSList.items():\n",
    "    statArr.append(value)\n",
    "\n",
    "#now convert the regular array to the numpy array. This will allow to apply\n",
    "#the numpy operations\n",
    "nparray=np.asarray(statArr)\n",
    "\n",
    "#find the max value of occurence of a key (ngram)\n",
    "maxVal=np.amax(nparray)\n",
    "\n",
    "#find the min value of occurence of a key (ngram)\n",
    "minVal=np.amin(nparray)\n",
    "print(maxVal,\"....\",minVal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using pandas to see the output in tabular form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    num of occurence  Number of such Keys\n",
      "0                 10                    9\n",
      "1                 11                   13\n",
      "2                 12                   14\n",
      "3                 13                    8\n",
      "4                 14                    5\n",
      "5                 15                    8\n",
      "6                 16                    5\n",
      "7                 17                    4\n",
      "8                 18                    2\n",
      "9                 19                    5\n",
      "10                20                    3\n",
      "11                21                    4\n",
      "12                22                    1\n",
      "13                24                    3\n",
      "14                25                    1\n",
      "15                27                    1\n",
      "16                28                    1\n",
      "17                33                    1\n",
      "18                39                    1\n",
      "19                40                    2\n",
      "20                42                    2\n",
      "21                47                    1\n",
      "22                49                    1\n",
      "23                53                    1\n",
      "24                62                    1\n",
      "25                71                    1\n",
      "26                89                    1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/305015992/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:7: FutureWarning: sort(....) is deprecated, use sort_index(.....)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def getFreqDistribution():\n",
    "    #this will give the frequency count\n",
    "    counts,unique = np.unique(nparray, return_counts=True)\n",
    "    pp=pd.DataFrame(list(zip(counts,unique)),columns=['num of occurence','Number of such Keys'])\n",
    "    print(pp.sort())\n",
    "\n",
    "getFreqDistribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that only 1 key has occured 89 times in the corpus. TO find out which is that key we have the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following are the keys which have occured 89 times\n",
      "lube oil pressure\n",
      "Following are the keys which have occured 10 times\n",
      "stator coil vib\n",
      "inlet filter dp\n",
      "exhaust duct pressure\n",
      "compressor outlet temp range\n",
      "driver thrust bearing\n",
      "agb chip detector\n",
      "sh dsh outlet temperature\n",
      "fw pmp mtr outboard vib\n",
      "bearing oil drain temperature\n"
     ]
    }
   ],
   "source": [
    "def getKeysBasedOnFrequency(freq):\n",
    "    indexes=[]\n",
    "    for i in range(len(nparray)):\n",
    "        if (nparray[i]==freq):\n",
    "            print(list(finalPOSList.keys())[i])\n",
    "\n",
    "print(\"Following are the keys which have occured 89 times\")                \n",
    "getKeysBasedOnValue(89)\n",
    "\n",
    "print(\"Following are the keys which have occured 10 times\")\n",
    "getKeysBasedOnValue(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the keys to a seperate array so that we can perform some more operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngramList=[]\n",
    "#print only the keys i.e. the name of the ngram\n",
    "for key,value in finalPOSList.items():\n",
    "    ngramList.append(key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding additional ngrams\n",
    "Now we want to find out the additional ngrams that we have obtained using the spacy. For this we have to compare with the original dictionary (assume we already have a dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictFile=\"keywordMapping_jim.csv\"\n",
    "dictLines = [dictLine.rstrip('\\n').split(',') for dictLine in open(dictFile)]\n",
    "unigramDict={}\n",
    "ngramDict={}\n",
    "for ll in dictLines:\n",
    "    #The ngramdict will contain normalized version both side since the \n",
    "    #assumption is when we are testing for the ngram the text is already \n",
    "    #normalized to unigrams\n",
    "    if(len(ll[2].split())>1):\n",
    "        str=ll[2]\n",
    "        str=str.replace('\"','')        \n",
    "        ngramDict[str]=str\n",
    "    else:\n",
    "        str=ll[1]\n",
    "        str=str[1:-1]\n",
    "        unigramDict[str]=ll[2]\n",
    "        #unigramDict[str].replace('\"','')\n",
    "#         ngramDict[str]=ll[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility function to check if a word exists in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"valve\"\n"
     ]
    }
   ],
   "source": [
    "def my_in_array(word,dictArr):\n",
    "    for idx,key in enumerate(dictArr):\n",
    "        if(key==word):\n",
    "            return(dictArr[key])\n",
    "print(my_in_array(\"vlv\",unigramDict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to update the dictionary. Here is the approach that I have followed\n",
    "1. check which all unigrams are not in the dictionary. Add those unigrams in the dictionary\n",
    "2. Normalize all the ngrams (convert pmp brg mtl to pump bearing metal)\n",
    "3. Check for the presence of this normalize ngram in the dictionary\n",
    "4. If not present then it needs to be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#notInPrevDict will keep a track of the keys which are new\n",
    "notInPrevDict=[]\n",
    "for uu in ngramList:\n",
    "    #uu=ngramList[1]\n",
    "    words=uu.split(\" \")\n",
    "    for word in words:\n",
    "        if(len(word)>1):\n",
    "            retword=my_in_array(word,unigramDict)\n",
    "            if(retword==None):\n",
    "                #print(word)\n",
    "                #add to the unigram dictionary\n",
    "                notInPrevDict.append(word)\n",
    "                unigramDict[word]=word    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(unigramDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#normalize the ngramList\n",
    "normalizedNGramList=[]\n",
    "for nn in ngramList:\n",
    "    words = nn.split(\" \")\n",
    "    #print(words)\n",
    "    str=''\n",
    "    for word in words:\n",
    "        retword=my_in_array(word,unigramDict)\n",
    "        if(retword):\n",
    "            retword = retword.replace('\"', '')\n",
    "            str += retword + \" \"\n",
    "    #print(str)\n",
    "    normalizedNGramList.append(str.strip())\n",
    "#print(normalizedNGramList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(notInPrevDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check which all ngrams are present in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for nn in normalizedNGramList:\n",
    "    retword=my_in_array(nn,ngramDict)\n",
    "    if(retword==None):\n",
    "        notInPrevDict.append(nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['duct burner fuel gas flow', 'power turbine position', 'hrsg drum spec cndc sample', 'pump ib bearing temperature', 'siemens tower drive metal temperature', 'fw pump motor outboard vibration']\n"
     ]
    }
   ],
   "source": [
    "#these are the ones that needs to be added\n",
    "print(notInPrevDict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
