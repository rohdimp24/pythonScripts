{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This script is meant to generate the dictionary automatically using the N-Gram Analysis\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm steps:\n",
    "1. Read the cases, stop words, domain words\n",
    "2. Normalize the cases by removing the stop words and the punctuations\n",
    "3. Using the countVectorizer find out the tokens and the frequency\n",
    "4. Take the tokens which are quite frequent\n",
    "5. These are the unigrams\n",
    "6. Now break the normalised cases in the step 2 and create the bigrams\n",
    "7. take the most frequent bigrams\n",
    "8. Repeat the step 6 and 7 to get the trigrams\n",
    "9. Now normalize the unigrams so that increasingly, increased, increases, increase all map to increase\n",
    "10. Use the domain words to further normalise the unigrams so that rot is mapped to rotor\n",
    "11. Using the normaized unigrams we need to normalize the bigrams and trigrams so that \"brg mtl temp\" becomes \"bearing metal temperature\"\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the data (later it should be through a database connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize():\n",
    "\n",
    "    #prefixpath=\"/Users/305015992/pythonProjects/wordcloud/\"\n",
    "    \n",
    "    #Read teh cases file. This is the raw cases containing the full data and not split at the sentence level\n",
    "    fname=\"all_jim_case_large.txt\"\n",
    "    with open(fname, 'r') as myfile:\n",
    "        data=myfile.read()\n",
    "    data=data.split('-------BREAK--------')\n",
    "    cases=[case.strip() for case in data]\n",
    "    print(cases[1:10])\n",
    "    print(len(cases))\n",
    "    lengthCases=len(cases)\n",
    "\n",
    "    #read the stop words list\n",
    "    stopwordsFile = open('stopwordsss.txt', 'r')\n",
    "    stopwords=stopwordsFile.read()\n",
    "    stopwordList=stopwords.split(\",\")\n",
    "\n",
    "    #list of the domain words\n",
    "    domainFileName='domainss.txt'\n",
    "    domainLines = [domainLine.rstrip('\\n').split(',') for domainLine in open(domainFileName)]\n",
    "    domainDict={}\n",
    "    for dl in domainLines:\n",
    "            key=dl[0].replace('\"','')\n",
    "            value=dl[1].replace('\"','')\n",
    "            domainDict[key]=value.strip()\n",
    "   \n",
    "    return cases,stopwordList,domainDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data and initialize the varaible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It appears while the turbine is active power is moving around the wind speed sensor is returning a flatline value.\\n\\nUpdate 1-3-14: Appears the Anemometer issues have been resolved as of 1-2-14 at 7:00am', 'Since 11/25/14, the following tags are indicating intermittent flatlines: IDO WTG001 MDK030 BT001:XQ01.M_HSRot_Tmp, IDO WTG001 MDK030 BT002:XQ01.M_IMSGen_Tmp, and IDO WTG001 MDK030 BT003:XQ01.M_IMSRot_Tmp Actual temperatures are acceptable when tags are not flatlining (~65 degC). Update 1/26/15: Flatlines cleared. Closing case.', 'Beginning Aug 8, wind turbine 1 appears curtailed at ~2700 kW with wind speed between ~4-13 m/s.', 'During higher wind conditions the turbine is currently limited to 1.8 MW', 'Since 2/23/15, wind turbine 2 appears to be curtailed at ~2700 kW with wind speed between ~5-21 m/s.  Update 4/13/15: Wind turbine 2 appears to be curtailed since 4/10. Currently, active power is ~2700 kW.  Curtailment cleared.', 'On 11/27/14, generator bearing temperature remained above the model estimate for ~14 hours.The maximum temperature observed was ~83 degC with a model estimate of ~62 degC.  Update 4/13/15: The condition persists with a high temperature of ~78C observed on 4/9.', 'Within the past week, DE bearing temperatures have been tracking ~10-20C above the model, as high as ~84C. No other deviations are noted in the model.', 'Within the past week, generator bearing temperature DE has exceeded the model estimate with a recent increase as high as ~79C with an estimate of ~60C. The turbine was running at lower power when most of the increases occurred at <2000kW and generator speed was <1500 RPM. Ambient temperature was ~3-4C during this time.', 'While the unit is operating at decreased power <2000 kW, generator bearing temperature DE has been elevated running as high as ~86C. Ambient is ~15-18C during this time. Other wind turbines during the same timeframe are operating with DE temperatures ~50-60C with similar active power.']\n",
      "11901\n"
     ]
    }
   ],
   "source": [
    "cases,stopwordList,domainDict=initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleanup the cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def cleaupCaseData(cases):\n",
    "    lengthCases=len(cases)\n",
    "    cleanedUpCases=[]\n",
    "    for count in range(0,lengthCases):\n",
    "        # print(\"before {}\",case)\n",
    "        case=cases[count]\n",
    "        case=case.lower()\n",
    "        case = case.strip();\n",
    "        case = re.sub('/[^A-Za-z0-9 _\\-\\+\\&\\,\\#]/', '', case)\n",
    "        case = case.replace('\"', ' ')\n",
    "        case = case.replace('\\\"', ' ')\n",
    "        case = case.replace('>', ' ')\n",
    "        case = case.replace('@', ' ')\n",
    "        case = case.replace('<', ' ')\n",
    "        case = case.replace(':', ' ')\n",
    "        case = case.replace('.', ' ')\n",
    "        case = case.replace('(', ' ')\n",
    "        case = case.replace(')', ' ')\n",
    "        case = case.replace('[', ' ')\n",
    "        case = case.replace(']', ' ')\n",
    "        case = case.replace('_', ' ')\n",
    "        case = case.replace(',', ' ')\n",
    "        case = case.replace('#', ' ')\n",
    "        case = case.replace('-', ' ')\n",
    "        case = case.replace('/', ' ')\n",
    "        case = case.replace('\"', ' ')\n",
    "        case = case.replace('\\n', ' ')\n",
    "        case = case.replace('~', ' ')\n",
    "        case = re.sub(r'\\d+', ' ', case)\n",
    "        cleanedUpCases.append(case)\n",
    "    return cleanedUpCases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it appears while the turbine is active power is moving around the wind speed sensor is returning a flatline value   update        appears the anemometer issues have been resolved as of       at    am', 'since        the following tags are indicating intermittent flatlines  ido wtg  mdk  bt  xq  m hsrot tmp  ido wtg  mdk  bt  xq  m imsgen tmp  and ido wtg  mdk  bt  xq  m imsrot tmp actual temperatures are acceptable when tags are not flatlining     degc   update        flatlines cleared  closing case ', 'beginning aug    wind turbine   appears curtailed at    kw with wind speed between      m s ', 'during higher wind conditions the turbine is currently limited to     mw', 'since        wind turbine   appears to be curtailed at    kw with wind speed between      m s   update        wind turbine   appears to be curtailed since      currently  active power is    kw   curtailment cleared ', 'on        generator bearing temperature remained above the model estimate for    hours the maximum temperature observed was    degc with a model estimate of    degc   update        the condition persists with a high temperature of   c observed on     ', 'within the past week  de bearing temperatures have been tracking     c above the model  as high as   c  no other deviations are noted in the model ', 'within the past week  generator bearing temperature de has exceeded the model estimate with a recent increase as high as   c with an estimate of   c  the turbine was running at lower power when most of the increases occurred at   kw and generator speed was    rpm  ambient temperature was     c during this time ', 'while the unit is operating at decreased power    kw  generator bearing temperature de has been elevated running as high as   c  ambient is     c during this time  other wind turbines during the same timeframe are operating with de temperatures     c with similar active power ']\n",
      "11901\n"
     ]
    }
   ],
   "source": [
    "cleanedUpCases=cleaupCaseData(cases)\n",
    "print(cleanedUpCases[1:10])\n",
    "print(len(cleanedUpCases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictioary of the words and the frequency count\n",
    "Vocabulary is list of the features obtained using the countVectorizer\n",
    "\n",
    "Using the dtm to do this\n",
    "Word->count across all the cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: (11901, 582)\n",
      "582\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(min_df=0.001,stop_words=stopwordList,strip_accents='unicode',binary=False)\n",
    "rawDtm = count_vect.fit_transform(cleanedUpCases)\n",
    "print(\"Data dimensions: {}\".format(rawDtm.shape))\n",
    "\n",
    "#vocabulary is the list of the feature names\n",
    "vocab=count_vect.get_feature_names()\n",
    "print(len(vocab))\n",
    "\n",
    "#convert the dtm to np array this will enable us to perform the colsum and rowsum\n",
    "countDtm = rawDtm.toarray()\n",
    "countDtm=np.array(countDtm)\n",
    "#calculate the frequecny sum for all the tokens\n",
    "freqsum=np.sum(countDtm,axis=0)\n",
    "\n",
    "# np.amax(freqsum)\n",
    "# np.amin(freqsum)\n",
    "\n",
    "\n",
    "freqDict={}\n",
    "for idx,v in enumerate(vocab):\n",
    "    #freqDict.append({'word':v,'count':freqsum[idx]})\n",
    "    freqDict[v]=freqsum[idx]\n",
    "#print(freqDict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addding POS words based on maxent algorithm \n",
    "1. read the pos distribution file (currently this is already supplied)\n",
    "2. We will pick only the nouns and pronouns\n",
    "3. we will compare the list of pos with the unigrams that we already have\n",
    "4. add the new words to the unigrams\n",
    "5. we will keep track of the frequency of the new unigrams as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "posWords=[]\n",
    "posWordsDict={}\n",
    "with open(\"posDist.csv\", 'r') as csvfile:\n",
    "    posReader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in posReader:\n",
    "        ch=row[7].replace('\"','')\n",
    "        if(ch==\"N\"):\n",
    "            str=row[1].replace('\"', '')\n",
    "            posWords.append(str)\n",
    "            posWordsDict[str]=row[3].replace('\"','')\n",
    "        if(ch==\"J\"):\n",
    "            str = row[1].replace('\"', '')\n",
    "            posWords.append(str)\n",
    "            posWordsDict[str] = row[2].replace('\"', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the utility function to check for the existence of the word in the list\n",
    "def my_in_array(word,wordlist):\n",
    "    # print(type(word))\n",
    "     for key in wordlist:\n",
    "        if(key==word):\n",
    "            return(True)\n",
    "     return(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ho 21\n",
      "angle 20\n",
      "desuperheater 15\n",
      "srv 18\n",
      "wsao 17\n",
      "inho 39\n",
      "brgx 13\n",
      "behaviour 14\n",
      "teg 15\n",
      "device 16\n",
      "damper 27\n",
      "cell 15\n",
      "fcv 15\n",
      "pulverizer 15\n",
      "calibration 13\n",
      "lcv 18\n",
      "reservoir 20\n",
      "trucks 14\n",
      "bed 16\n",
      "charge 16\n",
      "mark 14\n",
      "intensity 17\n",
      "horiz 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posWordsToBeAdded=[]\n",
    "#keep the threshold to the currently minimum frequency of the unigrams\n",
    "minCount=np.amin(freqsum)\n",
    "#print(minCount)\n",
    "for k in posWordsDict:\n",
    "    if (my_in_array(k,unigrams)==False):\n",
    "        if(my_in_array(k,stopwordList)==False):\n",
    "            if(int(posWordsDict[k])>minCount):\n",
    "                print(k,posWordsDict[k])\n",
    "                posWordsToBeAdded.append(k)\n",
    "                #add to the exiting frequecy dictionary of the unigram\n",
    "                freqDict[k]=int(posWordsDict[k])\n",
    "\n",
    "len(posWordsToBeAdded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the sorted list of the freqdict\n",
    "This is based on the suggestions given at http://stackoverflow.com/questions/613183/sort-a-python-dictionary-by-value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "sortedFreqDict=sorted(freqDict.items(), key=itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### utility functions to access the frequency of a keyword or given a frequecny which all keywords are mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "disch\n",
      "noise\n",
      "bearing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def getFreqOfKeyword(freqDict,keyword):\n",
    "    for key in freqDict:\n",
    "        if(key==keyword):\n",
    "            return(freqDict[key])\n",
    "\n",
    "\n",
    "def getKeywordBasedOnFrequency(freqDict,freq):\n",
    "    for key in freqDict:\n",
    "        if (freqDict[key]==freq):\n",
    "            return (key)\n",
    "\n",
    "\n",
    "\n",
    "print(getFreqOfKeyword(freqDict,\"disch\"))\n",
    "print(getKeywordBasedOnFrequency(freqDict,81))\n",
    "print(getKeywordBasedOnFrequency(freqDict,np.amin(freqsum)))\n",
    "print(getKeywordBasedOnFrequency(freqDict,np.amax(freqsum)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The unigrams are same as the keys of the freqDist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams=[]\n",
    "for k in freqDict:\n",
    "    unigrams.append(k)\n",
    "len(unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to find the bigrams\n",
    "We are using the nltk bigramcollocator for this purpose .. this is based on \n",
    "http://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python\n",
    "\n",
    "Basically for the given case it will find out the bigrams in that case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bigrams(myString):\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    tokens = tokenizer.tokenize(myString)\n",
    "    ''' stopwordlist is the one that we read in the initialize phase'''\n",
    "    cleanedTokens=[x for x in tokens if x.lower() not in stopwordList]\n",
    "    # print(tokens)\n",
    "    #stemmer = PorterStemmer()\n",
    "    bigram_finder = BigramCollocationFinder.from_words(cleanedTokens)\n",
    "    #bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 5)\n",
    "    #print(bigrams)\n",
    "\n",
    "    '''Returns the list of the bigrams for the case in the form of a dictioary of bigram and its count\n",
    "    e.g. dict_items([(('power', 'wind'), 1), (('speed', 'sensor'), 1), (('anemometer', 'issues'), 1)]),\n",
    "    '''\n",
    "    return bigram_finder.ngram_fd.items()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to find the trigrams \n",
    "this is again based on http://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_trigrams(myString):\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    tokens = tokenizer.tokenize(myString)\n",
    "    ''' stopwordlist is the one that we read in the initialize phase'''\n",
    "    cleanedTokens=[x for x in tokens if x.lower() not in stopwordList]\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(cleanedTokens)\n",
    "    #trigrams = trigram_finder.nbest(TrigramAssocMeasures.chi_sq, 5)\n",
    "   \n",
    "    '''Returns the list of the trigrams for the case in the form osf a dictioanry of trigram and its count'''\n",
    "    return trigram_finder.ngram_fd.items()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now find the bigrams using the get_bigrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11901\n",
      "31653\n",
      "732\n"
     ]
    }
   ],
   "source": [
    "'''for all the cases find out the bigrams'''\n",
    "bigramFeatures=[]\n",
    "for case in cleanedUpCases:\n",
    "    #need to make sure that the case does not have stop words\n",
    "    bigramFeatures.append(get_bigrams(case))\n",
    "\n",
    "print(len(bigramFeatures))\n",
    "\n",
    "\n",
    "'''Now combine the individual dictioary items together..also count the occurences of the same bigrams'''\n",
    "bigramCounts={}\n",
    "for bigramFeature in bigramFeatures:\n",
    "    for k, v in bigramFeature:\n",
    "        key=(' '.join(k))\n",
    "        if key not in bigramCounts.keys():\n",
    "            bigramCounts[key]=v\n",
    "        else:\n",
    "            bigramCounts[key]+=v\n",
    "\n",
    "print(len(bigramCounts))\n",
    "\n",
    "\n",
    "'''\n",
    "Now we will create teh final list of bigrams..they should have the following property\n",
    "1. The frequency of the bigram should be more than 20\n",
    "2. both the parts should be unigrams individually\n",
    "3. both the parts should not be same 'temp temp' is not valid\n",
    "'''\n",
    "finalBigramFeatures=[]\n",
    "for key in bigramCounts:\n",
    "    if(bigramCounts[key]>20):\n",
    "        #print(key)\n",
    "        arrBigrams=key.split()\n",
    "        if(arrBigrams[0]!=arrBigrams[1]):\n",
    "            #iff the individual words are part of unigrams then only add it\n",
    "            if((arrBigrams[0] in unigrams) and (arrBigrams[1] in unigrams)):\n",
    "                finalBigramFeatures.append(key)\n",
    "\n",
    "#finalBigramFeatures\n",
    "print(len(finalBigramFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### similarly for the trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11901\n",
      "60634\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "'''Get the dictioanries for individual cases'''\n",
    "trigramFeatures=[]\n",
    "for case in cleanedUpCases:\n",
    "    trigramFeatures.append(get_trigrams(case))\n",
    "print(len(trigramFeatures))\n",
    "\n",
    "'''Combine the dictionaries to get the counts of individual trigrams'''\n",
    "trigramCounts={}\n",
    "for trigramFeature in trigramFeatures:\n",
    "    for k, v in trigramFeature:\n",
    "        key=(' '.join(k))\n",
    "        if key not in trigramCounts.keys():\n",
    "            trigramCounts[key]=v\n",
    "        else:\n",
    "            trigramCounts[key]+=v\n",
    "\n",
    "print(len(trigramCounts))\n",
    "\n",
    "\n",
    "'''\n",
    "Now we will create teh final list of trigrams..they should have the following property\n",
    "1. The frequency of the trigrams should be more than 20\n",
    "2. all the parts should be unigrams individually\n",
    "3. all the parts should not be same 'temp temp temp' is not valid\n",
    "'''\n",
    "finalTrigramFeatures=[]\n",
    "for key in trigramCounts:\n",
    "    if(trigramCounts[key]>20):\n",
    "        #print(key)\n",
    "        arrTrigrams=key.split()\n",
    "        if(arrTrigrams[0]!=arrTrigrams[1]!=arrTrigrams[2]):\n",
    "            #iff the individual words are part of unigrams then only add it\n",
    "            if((arrTrigrams[0] in unigrams) and (arrTrigrams[1] in unigrams) and (arrTrigrams[2] in unigrams)):\n",
    "                finalTrigramFeatures.append(key)\n",
    "\n",
    "#finalBigramFeatures\n",
    "print(len(finalTrigramFeatures))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the spacy code for the bigrams and trigrams here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Utility function to cleanup the nounphrases'''\n",
    "\n",
    "def cleanupNounPhrase(str):\n",
    "    str = str.strip();\n",
    "    str = re.sub('/[^A-Za-z0-9 _\\-\\+\\&\\,\\#]/', '', str)\n",
    "    str = str.replace('\"', ' ')\n",
    "    str = str.replace('\\\"', ' ')\n",
    "    str = str.replace(')', ' ')\n",
    "    str = str.replace('(', ' ')\n",
    "    str = str.replace('>', ' ')\n",
    "    str = str.replace('@', ' ')\n",
    "    str = str.replace('<', ' ')\n",
    "    str = str.replace(':', ' ')\n",
    "    str = str.replace('.', ' ')\n",
    "    str = str.replace('[', ' ')\n",
    "    str = str.replace(']', ' ')\n",
    "    str = str.replace('_', ' ')\n",
    "    str = str.replace(',', ' ')\n",
    "    str = str.replace('#', ' ')\n",
    "    str = str.replace('-', ' ')\n",
    "    str = str.replace('/', ' ')\n",
    "    str = str.replace('\"', ' ')\n",
    "    str = str.replace('\\n', ' ')\n",
    "    str = str.replace('~', ' ')\n",
    "    str = re.sub(r'\\d+', ' ', str)\n",
    "    word_sent = [word for word in str.lower().split(\" \") if word not in stopwordList and len(word)>1]\n",
    "    if (len(word_sent) > 1):\n",
    "        finalSent = ' '.join(word_sent)\n",
    "    else:\n",
    "        finalSent=''\n",
    "\n",
    "    return(finalSent.strip())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11901\n",
      "after flatteing noun phrases 22785\n",
      "8182\n",
      "probe sample 10\n",
      "turb brg 11\n",
      "wheelspace differential 10\n",
      "rr notification 11\n",
      "hydrogen pressure 10\n",
      "diff press 11\n",
      "positive residuals 36\n",
      "lp drum 35\n",
      "steam silica 14\n",
      "bearing temps 68\n",
      "brg vib 24\n",
      "drive generator bearing temperature 11\n",
      "suction temp 10\n",
      "condensate pump 13\n",
      "wind speed 89\n",
      "wind turbines 16\n",
      "seal gas filter 15\n",
      "lube oil temp 40\n",
      "brg mtl temp 17\n",
      "turbine temperature wheelspace 16\n",
      "deep cut 19\n",
      "model prediction 317\n",
      "sh dsh outlet temperature 10\n",
      "gt load 10\n",
      "bearing vib 13\n",
      "motor ob bearing temp 12\n",
      "duct burner spread 27\n",
      "inlet temp 18\n",
      "vibration issues 15\n",
      "lo pressure 22\n",
      "temperature issues 16\n",
      "lube oil 22\n",
      "lp vib 12\n",
      "chip detectors 12\n",
      "condition persists 20\n",
      "nde bearing temperature 24\n",
      "axial position sensor 15\n",
      "water injection flow 20\n",
      "gen bearing 11\n",
      "unit start 76\n",
      "ib bearing temp 16\n",
      "motor winding temp 12\n",
      "metal temp 91\n",
      "erratic values 25\n",
      "thrust bearing temperature 19\n",
      "performance tag 25\n",
      "fw flow 19\n",
      "cylinder exhaust temperature 14\n",
      "lo temperature 22\n",
      "gen brg 13\n",
      "nde vibration 11\n",
      "hrsg drum 18\n",
      "bearing drain 10\n",
      "fw pmp mtr outboard vib 10\n",
      "motor current 88\n",
      "fuel gas pressure 18\n",
      "suction flow 19\n",
      "mtl temp 15\n",
      "differential pressure 22\n",
      "unit load 12\n",
      "generator bearing temperature de 15\n",
      "attemperator spray flow 15\n",
      "gt nox 13\n",
      "fuel gas 13\n",
      "stator temps 12\n",
      "steam temperature 13\n",
      "ammonia flow 78\n",
      "stator temp 21\n",
      "stator coil vib 10\n",
      "trained data 32\n",
      "lo temp 35\n",
      "motor stator 17\n",
      "driver thrust bearing 10\n",
      "duct burning 11\n",
      "duct burner fuel gas flow 16\n",
      "air compressor 10\n",
      "low values 12\n",
      "duct burner 25\n",
      "gearbox oil temp 20\n",
      "elevated vibrations 11\n",
      "inlet position 11\n",
      "ambient temps 21\n",
      "ip drum conductivity 11\n",
      "intercooler outlet temperature 14\n",
      "air flow 19\n",
      "axial positions 10\n",
      "gearbox oil 14\n",
      "gear bearing temp 13\n",
      "nde seal outlet flow 15\n",
      "disc cavity 17\n",
      "rear strut pressure 39\n",
      "ambient temp 38\n",
      "fuel gas temperature 15\n",
      "suction discharge 10\n",
      "hrsg drum spec cndc sample 19\n",
      "nde vibrations 24\n",
      "perf tag 12\n",
      "ws fi 14\n",
      "ip fw flow 11\n",
      "m&d center 17\n",
      "turbine outlet temp 12\n",
      "fuel gas temp 21\n",
      "generator bearing 57\n",
      "filter dp 31\n",
      "bearing metal 62\n",
      "generator speed 19\n",
      "wheel temp 11\n",
      "mech model 15\n",
      "ip blowdown silica 22\n",
      "outlet temp 15\n",
      "gearbox bearing temp 17\n",
      "nde bearing 17\n",
      "bearing vibrations 221\n",
      "agb chip detector 10\n",
      "rear gear bearing temp 12\n",
      "vibration sensor 41\n",
      "exhaust tc 107\n",
      "motor winding temperature 13\n",
      "exhaust thermocouple 18\n",
      "performance model 10\n",
      "shaft vibrations 14\n",
      "brg vibration 10\n",
      "temperature sensors 14\n",
      "low load 60\n",
      "vibration tags 31\n",
      "drum sensor 10\n",
      "erratic readings 17\n",
      "outlet temperature 14\n",
      "current conditions 13\n",
      "thrust bearing axial position 21\n",
      "strut pressure 17\n",
      "de seal dp 16\n",
      "axial displacement 14\n",
      "exhaust temperature 35\n",
      "erratic reading 15\n",
      "thermal mechanical tag 12\n",
      "discharge temperature 48\n",
      "lp steam 13\n",
      "brg metal temp 17\n",
      "gearbox oil temperature 12\n",
      "generator bearing metal temp 13\n",
      "drum cond 14\n",
      "gen bearing temp 53\n",
      "historical values 37\n",
      "discharge pressure 87\n",
      "rear gen bearing temp 49\n",
      "generator bearing temperature nde 47\n",
      "generator bearing temp 14\n",
      "seal oil pressure 13\n",
      "bearing metal temp 62\n",
      "drum conductivity 18\n",
      "mechanical model 12\n",
      "bearing vibration sensor 11\n",
      "unit run 122\n",
      "ip drum 84\n",
      "engine speed 12\n",
      "flame detector 23\n",
      "intercooler coolant pressure 13\n",
      "lube oil temperature 71\n",
      "driver load 13\n",
      "compressor outlet temp range 10\n",
      "suction pressure 49\n",
      "bearing temperature 80\n",
      "gas temp 10\n",
      "valve position 21\n",
      "seal pressure 10\n",
      "crankcase pressure 22\n",
      "bearing metal temps 21\n",
      "flatlined data 12\n",
      "exhaust spreads 20\n",
      "inlet pressure 20\n",
      "motor stator temperature 12\n",
      "generator de bearing temperature 13\n",
      "fw pmp axial probe 12\n",
      "compressor inlet pressure 40\n",
      "bowl dp 13\n",
      "gen slipring bearing vib 11\n",
      "gross load 84\n",
      "steam pressure 12\n",
      "ob bearing 15\n",
      "motor stator temps 11\n",
      "inactive thrust 46\n",
      "thrust bearing 34\n",
      "combustion monitor spread 11\n",
      "exhaust temp 77\n",
      "de bearing vibrations 12\n",
      "thrust position 68\n",
      "wheelspace temp 18\n",
      "drum differential 56\n",
      "exhaust duct pressure 10\n",
      "journal bearings 10\n",
      "thrust bearing position 12\n",
      "ip blowdown 12\n",
      "axial position 185\n",
      "metal temperature 93\n",
      "turbine speed 25\n",
      "water jacket inlet pressure 15\n",
      "current reading 15\n",
      "model predictions 176\n",
      "bearing vibes 12\n",
      "poor tracking 16\n",
      "water temperature 10\n",
      "lower load 25\n",
      "exhaust spread 86\n",
      "temperature differential 10\n",
      "rotor vib brg 12\n",
      "casing vibrations 10\n",
      "servo current 12\n",
      "historical data 28\n",
      "inboard vibration 10\n",
      "bearing metal temperature 42\n",
      "coal flow 26\n",
      "power turbine positions 14\n",
      "lube oil pressure 89\n",
      "ambient temperature 135\n",
      "prefilter fuel pressure 42\n",
      "de vibrations 34\n",
      "drain temp 33\n",
      "steam temp 23\n",
      "low alarm 16\n",
      "condenser pressure 17\n",
      "bearing temp 67\n",
      "spray flow 19\n",
      "steam turbine 29\n",
      "ip hrsg drum sample 19\n",
      "model estimate 623\n",
      "gas pressure 32\n",
      "discharge flow 32\n",
      "faulted data 19\n",
      "brg temp 22\n",
      "observed temperature 13\n",
      "adaptation fuel 14\n",
      "drive generator bearing temp 11\n",
      "sensor issue 38\n",
      "pressure steam tag 15\n",
      "fuel pressure 74\n",
      "lower ambient 16\n",
      "bearing oil drain temperature 10\n",
      "exhaust temp spread 11\n",
      "brg temps 18\n",
      "generator bearing temperature 14\n",
      "gear bearing 11\n",
      "combustion performance 12\n",
      "condenser backpressure 31\n",
      "flame scanner 12\n",
      "generator temp 10\n",
      "de bearing temperature 18\n",
      "lube oil filter dp 19\n",
      "fuel flow 13\n",
      "control valve 11\n",
      "de seal outlet flow 19\n",
      "lublind cooler temperature 12\n",
      "pump thrust 11\n",
      "unit restart 78\n",
      "lp steam flow 33\n",
      "motor temp 11\n",
      "wind turbine 76\n",
      "turbine exhaust pressure 16\n",
      "turbine bearing 29\n",
      "turbine bearing vibrations 20\n",
      "hydrogen gas pressure 11\n",
      "lube oil filter 21\n",
      "vibration readings 17\n",
      "mtl temps 11\n",
      "fuel gas flow 24\n",
      "suction temperature 67\n",
      "compressor speed 12\n",
      "shaft vibration 10\n",
      "priority notification 21\n",
      "load increase 14\n",
      "wheelspace temps 22\n",
      "hrsg temp 14\n",
      "bearing position 12\n",
      "duct firing 17\n",
      "steam flow 34\n",
      "siemens tower drive metal temperature 13\n",
      "winding temps 16\n",
      "bearing vibration 86\n",
      "gt vibration transducer 11\n",
      "rotor speed 13\n",
      "ob bearing temp 17\n",
      "pump ib brg temp 12\n",
      "coolant pressure 23\n",
      "exhaust temps 44\n",
      "inlet filter dp 10\n",
      "piston leak 17\n",
      "oil pressure 24\n",
      "fuel temperature 14\n",
      "ip drum differential 25\n",
      "oil drain temperature 11\n",
      "residual thresholds 18\n",
      "discharge temp 19\n",
      "model estimates 235\n",
      "exhaust gas port 12\n",
      "performance fuel 33\n",
      "exhaust pressure 12\n",
      "thrust bearing temp 28\n",
      "motor stator temp 24\n",
      "exhaust steam temperature 11\n",
      "oil drain temp 13\n",
      "perf model 18\n",
      "metal temps 21\n",
      "mechanical tag 33\n",
      "303\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from spacy.en import English\n",
    "nlp = English()\n",
    "\n",
    "docs = [nlp(d) for d in cases]\n",
    "\n",
    "'''Get the noun phrases'''\n",
    "arrNounPhrase=[]\n",
    "for idx, doc in enumerate(docs):\n",
    "    testDoc=doc\n",
    "    token_nounPhrases = [np.text for np in testDoc.noun_chunks]\n",
    "    updatedNounPhrases=[]\n",
    "    for tt in token_nounPhrases:\n",
    "        finalSent=cleanupNounPhrase(tt)\n",
    "        if(len(finalSent)>0):\n",
    "            updatedNounPhrases.append(finalSent)\n",
    "    arrNounPhrase.append(updatedNounPhrases)\n",
    "print(len(arrNounPhrase))\n",
    "\n",
    "'''\n",
    "Flattening the arrNounPhrase\n",
    "'''\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "flatNounPhraseList=flatten(arrNounPhrase)\n",
    "print(\"after flatteing noun phrases\",len(flatNounPhraseList))\n",
    "\n",
    "'''Now we need to tabulate the results'''\n",
    "nounPhraseDictCount={}\n",
    "import re\n",
    "#now we can apply the cleanup\n",
    "for tt in flatNounPhraseList:\n",
    "    finalSent=tt\n",
    "    if (nounPhraseDictCount.get(finalSent)):\n",
    "        nounPhraseDictCount[finalSent] += 1\n",
    "    else:\n",
    "        nounPhraseDictCount[finalSent] = 1\n",
    "\n",
    "print(len(nounPhraseDictCount))\n",
    "\n",
    "'''out of all the nounphrases pick up the once which are more frequncet'''\n",
    "finalPOSList={}\n",
    "for dd in nounPhraseDictCount:\n",
    "    ddArr=dd.split(\" \")\n",
    "    if(len(ddArr)>1):\n",
    "        #atelaset 10 occurence of the phrase\n",
    "        if(nounPhraseDictCount[dd]>9):\n",
    "            #print(dd,nounPhraseDictCount[dd])\n",
    "            finalPOSList[dd]=nounPhraseDictCount[dd]\n",
    "            #finalPOSList.append()\n",
    "\n",
    "print(len(finalPOSList))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "['elevated vibrations', 'pressure steam tag', 'nde seal outlet flow', 'cylinder exhaust temperature', 'seal oil pressure', 'rr notification', 'vibration issues', 'turbine outlet temp', 'inlet position', 'agb chip detector', 'driver thrust bearing', 'gen slipring bearing vib', 'gt vibration transducer', 'lube oil filter dp', 'rotor vib brg', 'casing vibrations', 'mechanical model', 'de seal outlet flow', 'exhaust duct pressure', 'rear gen bearing temp', 'observed temperature', 'power turbine positions', 'nde vibration', 'air compressor', 'pump ib brg temp', 'driver load', 'turbine exhaust pressure', 'water jacket inlet pressure', 'probe sample', 'intercooler coolant pressure', 'wheelspace differential', 'generator de bearing temperature', 'compressor outlet temp range', 'fw pmp axial probe', 'gearbox oil temperature', 'current conditions', 'motor winding temperature', 'rotor speed', 'compressor speed', 'motor ob bearing temp', 'axial positions', 'intercooler outlet temperature', 'thrust bearing axial position', 'hydrogen pressure', 'combustion monitor spread', 'journal bearings', 'gt load', 'motor temp', 'bowl dp', 'motor stator temps', 'drive generator bearing temp', 'generator bearing metal temp', 'inboard vibration', 'fuel temperature', 'stator coil vib', 'hrsg temp', 'residual thresholds', 'de seal dp', 'drive generator bearing temperature', 'axial displacement', 'wind turbines', 'bearing oil drain temperature', 'exhaust steam temperature', 'lp vib', 'chip detectors', 'exhaust temp spread', 'hydrogen gas pressure', 'mech model', 'temperature issues', 'duct firing', 'low alarm', 'engine speed', 'ip hrsg drum sample', 'seal gas filter', 'rear gear bearing temp', 'generator bearing temperature de', 'fuel gas temperature', 'de bearing vibrations', 'sh dsh outlet temperature', 'mtl temps', 'attemperator spray flow', 'gt nox', 'generator bearing temperature nde', 'thermal mechanical tag', 'current reading']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "need to further check 2 things \n",
    "1. Each of the individual word is a unigrams\n",
    "2. The nounphrase is not already present in the bigrams or trigrams \n",
    "'''\n",
    "\n",
    "listPOSNounPhrases=[]\n",
    "for k in finalPOSList:\n",
    "    listPOSNounPhrases.append(k)\n",
    "\n",
    "finallyCheckedNounPhrases=[] \n",
    "finalQuadgramFeatures=[]\n",
    "for key in listPOSNounPhrases:\n",
    "    arrNgrams=key.split()\n",
    "    flag=True\n",
    "    length=len(arrNgrams)\n",
    "    #print(key)\n",
    "    for i in range(length):\n",
    "        #print(i)\n",
    "        if(arrNgrams[i] not in unigrams):\n",
    "            flag=False\n",
    "                \n",
    "    if (flag==True):\n",
    "        if(length==2):\n",
    "            #check the presence in bigrams\n",
    "            if(key not in finalBigramFeatures):\n",
    "                finallyCheckedNounPhrases.append(key)\n",
    "                finalBigramFeatures.append(key)\n",
    "        if(length==3):\n",
    "            #check the presence in trigrams\n",
    "            if(key not in finalTrigramFeatures):\n",
    "                finallyCheckedNounPhrases.append(key)\n",
    "                finalTrigramFeatures.append(key)\n",
    "        if(length==4):\n",
    "            finallyCheckedNounPhrases.append(key)\n",
    "            finalQuadgramFeatures.append(key)\n",
    "\n",
    "print(len(finallyCheckedNounPhrases))\n",
    "print(finallyCheckedNounPhrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram length-> 605\n",
      "bigrams length-> 769\n",
      "trigrams length-> 252\n",
      "quadgrams length-> 21\n"
     ]
    }
   ],
   "source": [
    "'''Following is the length of variosu ngrams'''\n",
    "print(\"unigram length->\",len(unigrams))\n",
    "print(\"bigrams length->\",len(finalBigramFeatures))\n",
    "print(\"trigrams length->\",len(finalTrigramFeatures))\n",
    "print(\"quadgrams length->\",len(finalQuadgramFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now have to normalize the ngrams using their stemmed version\n",
    "\n",
    "The steps are as follows\n",
    "1. First create a map of stem and all the words that match this stem \n",
    "e.g. 'increas': ('increase', 895, 'increasing', 676, 'increases', 313)\n",
    "\n",
    "2. Create a word to stem mapping. \n",
    "e.g increase: increas\n",
    "\n",
    "3. Create a map of final stem to word map that is all the words that match this stem should be replaced by this word e.g.\n",
    "\n",
    "increas : increase\n",
    "\n",
    "decreas: decreased\n",
    "\n",
    "    This will use the map that was created in step 1\n",
    "\n",
    "4. now use the maps in step 2 and 3 to find the final dictionary\n",
    "\n",
    "word1->stem\n",
    "stem->normalizedword\n",
    "\n",
    "will give\n",
    "\n",
    "word1->nomralized word\n",
    "\n",
    "increasing : increase\n",
    "increases : increase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479\n",
      "479\n",
      "605\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "''' \n",
    "Create a dictioanry of stem-> all words\n",
    "{ 'decreas': ('decreased', 1149, 'decrease', 290, 'decreasing', 203, 'decreases', 91),...}\n",
    "'''\n",
    "stemmingDictBasedOnFreq={}\n",
    "for k,v in sortedFreqDict:\n",
    "    stemKey=stemmer.stem(k)\n",
    "    if (stemKey not in stemmingDictBasedOnFreq):\n",
    "        stemmingDictBasedOnFreq[stemKey]=(k,v)\n",
    "    else:\n",
    "        stemmingDictBasedOnFreq[stemKey]+=(k,v)\n",
    "\n",
    "print(len(stemmingDictBasedOnFreq))\n",
    "\n",
    "'''\n",
    "Create teh stem to word mapping. For all the stems that were found earlier we need to find which word will replace it \n",
    "THis word will be selected from the list of words that map to the same stem\n",
    "increas : increase\n",
    "\n",
    "'''\n",
    "stemmingDictFinal={}\n",
    "for k in stemmingDictBasedOnFreq:\n",
    "    #print(k)\n",
    "    #take the fisrst entry from the list that is stored for each stem value\n",
    "    stemmingDictFinal[k]=list(stemmingDictBasedOnFreq[k])[0]\n",
    "\n",
    "print(len(stemmingDictFinal))\n",
    "\n",
    "'''\n",
    "Create a word to stem mapping. For each unigram it will be created\n",
    "'''\n",
    "#word->stem mapping\n",
    "wordToStemMapping={}\n",
    "for k in unigrams:\n",
    "    #get the stem of the word\n",
    "    stemKey=stemmer.stem(k)\n",
    "    #create a map of word to its stem\n",
    "    wordToStemMapping[k]=stemKey\n",
    "\n",
    "print(len(wordToStemMapping))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now get the normalized version of each unigram using the stems map\n",
    "We will also make use of the somain information to cheange some of the unigrams like rot to rotor that cannot be done using the stem mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605\n",
      "pump\n"
     ]
    }
   ],
   "source": [
    "normalizedUnigramsDict={}\n",
    "for k in wordToStemMapping:\n",
    "    stem=wordToStemMapping[k]\n",
    "   #print(k,stem,stemmingDictFinal[stem])\n",
    "    normalizedUnigramsDict[k]=stemmingDictFinal[stem]\n",
    "\n",
    "print(len(normalizedUnigramsDict))\n",
    "\n",
    "#bring in the domain information as well\n",
    "for k in normalizedUnigramsDict:\n",
    "    for j in domainDict:\n",
    "        if (k==j):\n",
    "           # print(k,j,domainDict[j])\n",
    "            normalizedUnigramsDict[k]=domainDict[j]\n",
    "\n",
    "print(normalizedUnigramsDict['pmp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize bigrams and trigrams\n",
    "For the bigrams and trigrams it is basically normalizing the individual parts of the ngram\n",
    "Create a generic function for the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Takes the following\n",
    "ngramListToNormalize: The bigram or trigram input list to normalize\n",
    "normalizedUnigramDict: The unigram normalized dictionary containing raw->normalized unigram\n",
    "N: ngrams n\n",
    "'''\n",
    "def createNormalizedWordDict(ngramListToNormalize,normalizedUnigramDict,N):\n",
    "    normalizedNgramsDict={}\n",
    "    for k in ngramListToNormalize:\n",
    "        arrgrams=k.split()\n",
    "        str=''\n",
    "        for i in range(N):\n",
    "            str+=' '+normalizedUnigramDict[arrgrams[i]]\n",
    "        normalizedNgramsDict[k]=str.strip()\n",
    "    return normalizedNgramsDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769 769\n",
      "252 252\n",
      "21 21\n"
     ]
    }
   ],
   "source": [
    "normalizedBigramsDict=createNormalizedWordDict(finalBigramFeatures,normalizedUnigramsDict,2)\n",
    "normalizedTrigramsDict=createNormalizedWordDict(finalTrigramFeatures,normalizedUnigramsDict,3)\n",
    "normalizedQuadgramsDict=createNormalizedWordDict(finalQuadgramFeatures,normalizedUnigramsDict,4)\n",
    "\n",
    "print(len(finalBigramFeatures), len(normalizedBigramsDict))\n",
    "print(len(finalTrigramFeatures), len(normalizedTrigramsDict))\n",
    "print(len(finalQuadgramFeatures), len(normalizedQuadgramsDict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the dictioaries together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1647\n"
     ]
    }
   ],
   "source": [
    "finalNgramDict=normalizedUnigramsDict.copy()\n",
    "finalNgramDict.update(normalizedBigramsDict)\n",
    "finalNgramDict.update(normalizedTrigramsDict)\n",
    "finalNgramDict.update(normalizedQuadgramsDict)\n",
    "\n",
    "print(len(finalNgramDict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump the dictionary to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option1: pickle the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "output = open('outputDict.txt', 'ab+')\n",
    "pickle.dump(finalNgramDict, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option2: write to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('dict.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for key in finalNgramDict:\n",
    "        writer.writerow([key,finalNgramDict[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for k,v in finalNgramDict.items():\n",
    "#     if (k==\"metal temp reading\"):\n",
    "#         print(k,v)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
