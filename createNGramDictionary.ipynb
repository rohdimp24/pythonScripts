{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This script is meant to generate the dictionary automatically using the N-Gram Analysis\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm steps:\n",
    "1. Read the cases, stop words, domain words\n",
    "2. Normalize the cases by removing the stop words and the punctuations\n",
    "3. Using the countVectorizer find out the tokens and the frequency\n",
    "4. Take the tokens which are quite frequent\n",
    "5. These are the unigrams\n",
    "6. Now break the normalised cases in the step 2 and create the bigrams\n",
    "7. take the most frequent bigrams\n",
    "8. Repeat the step 6 and 7 to get the trigrams\n",
    "9. Now normalize the unigrams so that increasingly, increased, increases, increase all map to increase\n",
    "10. Use the domain words to further normalise the unigrams so that rot is mapped to rotor\n",
    "11. Using the normaized unigrams we need to normalize the bigrams and trigrams so that \"brg mtl temp\" becomes \"bearing metal temperature\"\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the data (later it should be through a database connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize():\n",
    "\n",
    "    #prefixpath=\"/Users/305015992/pythonProjects/wordcloud/\"\n",
    "    \n",
    "    #Read teh cases file. This is the raw cases containing the full data and not split at the sentence level\n",
    "    fname=\"all_jim_case_large.txt\"\n",
    "    with open(fname, 'r') as myfile:\n",
    "        data=myfile.read()\n",
    "    data=data.split('-------BREAK--------')\n",
    "    cases=[case.strip() for case in data]\n",
    "    print(cases[1:10])\n",
    "    print(len(cases))\n",
    "    lengthCases=len(cases)\n",
    "\n",
    "    #read the stop words list\n",
    "    stopwordsFile = open('stopwordsss.txt', 'r')\n",
    "    stopwords=stopwordsFile.read()\n",
    "    stopwordList=stopwords.split(\",\")\n",
    "\n",
    "    #list of the domain words\n",
    "    domainFileName='domainss.txt'\n",
    "    domainLines = [domainLine.rstrip('\\n').split(',') for domainLine in open(domainFileName)]\n",
    "    domainDict={}\n",
    "    for dl in domainLines:\n",
    "            key=dl[0].replace('\"','')\n",
    "            value=dl[1].replace('\"','')\n",
    "            domainDict[key]=value.strip()\n",
    "   \n",
    "    return cases,stopwordList,domainDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data and initialize the varaible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It appears while the turbine is active power is moving around the wind speed sensor is returning a flatline value.\\n\\nUpdate 1-3-14: Appears the Anemometer issues have been resolved as of 1-2-14 at 7:00am', 'Since 11/25/14, the following tags are indicating intermittent flatlines: IDO WTG001 MDK030 BT001:XQ01.M_HSRot_Tmp, IDO WTG001 MDK030 BT002:XQ01.M_IMSGen_Tmp, and IDO WTG001 MDK030 BT003:XQ01.M_IMSRot_Tmp Actual temperatures are acceptable when tags are not flatlining (~65 degC). Update 1/26/15: Flatlines cleared. Closing case.', 'Beginning Aug 8, wind turbine 1 appears curtailed at ~2700 kW with wind speed between ~4-13 m/s.', 'During higher wind conditions the turbine is currently limited to 1.8 MW', 'Since 2/23/15, wind turbine 2 appears to be curtailed at ~2700 kW with wind speed between ~5-21 m/s.  Update 4/13/15: Wind turbine 2 appears to be curtailed since 4/10. Currently, active power is ~2700 kW.  Curtailment cleared.', 'On 11/27/14, generator bearing temperature remained above the model estimate for ~14 hours.The maximum temperature observed was ~83 degC with a model estimate of ~62 degC.  Update 4/13/15: The condition persists with a high temperature of ~78C observed on 4/9.', 'Within the past week, DE bearing temperatures have been tracking ~10-20C above the model, as high as ~84C. No other deviations are noted in the model.', 'Within the past week, generator bearing temperature DE has exceeded the model estimate with a recent increase as high as ~79C with an estimate of ~60C. The turbine was running at lower power when most of the increases occurred at <2000kW and generator speed was <1500 RPM. Ambient temperature was ~3-4C during this time.', 'While the unit is operating at decreased power <2000 kW, generator bearing temperature DE has been elevated running as high as ~86C. Ambient is ~15-18C during this time. Other wind turbines during the same timeframe are operating with DE temperatures ~50-60C with similar active power.']\n",
      "11901\n"
     ]
    }
   ],
   "source": [
    "cases,stopwordList,domainDict=initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleanup the cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def cleaupCaseData(cases):\n",
    "    lengthCases=len(cases)\n",
    "    cleanedUpCases=[]\n",
    "    for count in range(0,lengthCases):\n",
    "        # print(\"before {}\",case)\n",
    "        case=cases[count]\n",
    "        case=case.lower()\n",
    "        case = case.strip();\n",
    "        case = re.sub('/[^A-Za-z0-9 _\\-\\+\\&\\,\\#]/', '', case)\n",
    "        case = case.replace('\"', ' ')\n",
    "        case = case.replace('\\\"', ' ')\n",
    "        case = case.replace('>', ' ')\n",
    "        case = case.replace('@', ' ')\n",
    "        case = case.replace('<', ' ')\n",
    "        case = case.replace(':', ' ')\n",
    "        case = case.replace('.', ' ')\n",
    "        case = case.replace('(', ' ')\n",
    "        case = case.replace(')', ' ')\n",
    "        case = case.replace('[', ' ')\n",
    "        case = case.replace(']', ' ')\n",
    "        case = case.replace('_', ' ')\n",
    "        case = case.replace(',', ' ')\n",
    "        case = case.replace('#', ' ')\n",
    "        case = case.replace('-', ' ')\n",
    "        case = case.replace('/', ' ')\n",
    "        case = case.replace('\"', ' ')\n",
    "        case = case.replace('\\n', ' ')\n",
    "        case = case.replace('~', ' ')\n",
    "        case = re.sub(r'\\d+', ' ', case)\n",
    "        cleanedUpCases.append(case)\n",
    "    return cleanedUpCases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleanedUpCases=cleaupCaseData(cases)\n",
    "#print(cleanedUpCases[1:10])\n",
    "#print(len(cleanedUpCases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictioary of the words and the frequency count\n",
    "Vocabulary is list of the features obtained using the countVectorizer\n",
    "\n",
    "Using the dtm to do this\n",
    "Word->count across all the cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: (11901, 582)\n",
      "582\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(min_df=0.001,stop_words=stopwordList,strip_accents='unicode',binary=False)\n",
    "rawDtm = count_vect.fit_transform(cleanedUpCases)\n",
    "print(\"Data dimensions: {}\".format(rawDtm.shape))\n",
    "\n",
    "#vocabulary is the list of the feature names\n",
    "vocab=count_vect.get_feature_names()\n",
    "print(len(vocab))\n",
    "\n",
    "#convert the dtm to np array this will enable us to perform the colsum and rowsum\n",
    "countDtm = rawDtm.toarray()\n",
    "countDtm=np.array(countDtm)\n",
    "#calculate the frequecny sum for all the tokens\n",
    "freqsum=np.sum(countDtm,axis=0)\n",
    "\n",
    "# np.amax(freqsum)\n",
    "# np.amin(freqsum)\n",
    "\n",
    "\n",
    "freqDict={}\n",
    "for idx,v in enumerate(vocab):\n",
    "    #freqDict.append({'word':v,'count':freqsum[idx]})\n",
    "    freqDict[v]=freqsum[idx]\n",
    "#print(freqDict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addding POS words based on maxent algorithm \n",
    "1. read the pos distribution file (currently this is already supplied)\n",
    "2. We will pick only the nouns and pronouns\n",
    "3. we will compare the list of pos with the unigrams that we already have\n",
    "4. add the new words to the unigrams\n",
    "5. we will keep track of the frequency of the new unigrams as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "posWords=[]\n",
    "posWordsDict={}\n",
    "with open(\"posDist.csv\", 'r') as csvfile:\n",
    "    posReader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in posReader:\n",
    "        ch=row[7].replace('\"','')\n",
    "        if(ch==\"N\"):\n",
    "            str=row[1].replace('\"', '')\n",
    "            posWords.append(str)\n",
    "            posWordsDict[str]=row[3].replace('\"','')\n",
    "        if(ch==\"J\"):\n",
    "            str = row[1].replace('\"', '')\n",
    "            posWords.append(str)\n",
    "            posWordsDict[str] = row[2].replace('\"', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the utility function to check for the existence of the word in the list\n",
    "def my_in_array(word,wordlist):\n",
    "    # print(type(word))\n",
    "     for key in wordlist:\n",
    "        if(key==word):\n",
    "            return(True)\n",
    "     return(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ho 21\n",
      "angle 20\n",
      "desuperheater 15\n",
      "srv 18\n",
      "wsao 17\n",
      "inho 39\n",
      "brgx 13\n",
      "behaviour 14\n",
      "teg 15\n",
      "device 16\n",
      "damper 27\n",
      "cell 15\n",
      "fcv 15\n",
      "pulverizer 15\n",
      "calibration 13\n",
      "lcv 18\n",
      "reservoir 20\n",
      "trucks 14\n",
      "bed 16\n",
      "charge 16\n",
      "mark 14\n",
      "intensity 17\n",
      "horiz 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posWordsToBeAdded=[]\n",
    "#keep the threshold to the currently minimum frequency of the unigrams\n",
    "minCount=np.amin(freqsum)\n",
    "#print(minCount)\n",
    "for k in posWordsDict:\n",
    "    if (my_in_array(k,unigrams)==False):\n",
    "        if(my_in_array(k,stopwordList)==False):\n",
    "            if(int(posWordsDict[k])>minCount):\n",
    "                print(k,posWordsDict[k])\n",
    "                posWordsToBeAdded.append(k)\n",
    "                #add to the exiting frequecy dictionary of the unigram\n",
    "                freqDict[k]=int(posWordsDict[k])\n",
    "\n",
    "len(posWordsToBeAdded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the sorted list of the freqdict\n",
    "This is based on the suggestions given at http://stackoverflow.com/questions/613183/sort-a-python-dictionary-by-value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "sortedFreqDict=sorted(freqDict.items(), key=itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### utility functions to access the frequency of a keyword or given a frequecny which all keywords are mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "disch\n",
      "noise\n",
      "bearing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def getFreqOfKeyword(freqDict,keyword):\n",
    "    for key in freqDict:\n",
    "        if(key==keyword):\n",
    "            return(freqDict[key])\n",
    "\n",
    "\n",
    "def getKeywordBasedOnFrequency(freqDict,freq):\n",
    "    for key in freqDict:\n",
    "        if (freqDict[key]==freq):\n",
    "            return (key)\n",
    "\n",
    "\n",
    "\n",
    "print(getFreqOfKeyword(freqDict,\"disch\"))\n",
    "print(getKeywordBasedOnFrequency(freqDict,81))\n",
    "print(getKeywordBasedOnFrequency(freqDict,np.amin(freqsum)))\n",
    "print(getKeywordBasedOnFrequency(freqDict,np.amax(freqsum)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The unigrams are same as the keys of the freqDist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams=[]\n",
    "for k in freqDict:\n",
    "    unigrams.append(k)\n",
    "len(unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to find the bigrams\n",
    "We are using the nltk bigramcollocator for this purpose .. this is based on \n",
    "http://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python\n",
    "\n",
    "Basically for the given case it will find out the bigrams in that case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bigrams(myString):\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    tokens = tokenizer.tokenize(myString)\n",
    "    ''' stopwordlist is the one that we read in the initialize phase'''\n",
    "    cleanedTokens=[x for x in tokens if x.lower() not in stopwordList]\n",
    "    # print(tokens)\n",
    "    #stemmer = PorterStemmer()\n",
    "    bigram_finder = BigramCollocationFinder.from_words(cleanedTokens)\n",
    "    #bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 5)\n",
    "    #print(bigrams)\n",
    "\n",
    "    '''Returns the list of the bigrams for the case in the form of a dictioary of bigram and its count\n",
    "    e.g. dict_items([(('power', 'wind'), 1), (('speed', 'sensor'), 1), (('anemometer', 'issues'), 1)]),\n",
    "    '''\n",
    "    return bigram_finder.ngram_fd.items()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to find the trigrams \n",
    "this is again based on http://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_trigrams(myString):\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    tokens = tokenizer.tokenize(myString)\n",
    "    ''' stopwordlist is the one that we read in the initialize phase'''\n",
    "    cleanedTokens=[x for x in tokens if x.lower() not in stopwordList]\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(cleanedTokens)\n",
    "    #trigrams = trigram_finder.nbest(TrigramAssocMeasures.chi_sq, 5)\n",
    "   \n",
    "    '''Returns the list of the trigrams for the case in the form osf a dictioanry of trigram and its count'''\n",
    "    return trigram_finder.ngram_fd.items()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now find the bigrams using the get_bigrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11901\n",
      "31653\n",
      "732\n"
     ]
    }
   ],
   "source": [
    "'''for all the cases find out the bigrams'''\n",
    "bigramFeatures=[]\n",
    "for case in cleanedUpCases:\n",
    "    #need to make sure that the case does not have stop words\n",
    "    bigramFeatures.append(get_bigrams(case))\n",
    "\n",
    "print(len(bigramFeatures))\n",
    "\n",
    "\n",
    "'''Now combine the individual dictioary items together..also count the occurences of the same bigrams'''\n",
    "bigramCounts={}\n",
    "for bigramFeature in bigramFeatures:\n",
    "    for k, v in bigramFeature:\n",
    "        key=(' '.join(k))\n",
    "        if key not in bigramCounts.keys():\n",
    "            bigramCounts[key]=v\n",
    "        else:\n",
    "            bigramCounts[key]+=v\n",
    "\n",
    "print(len(bigramCounts))\n",
    "\n",
    "\n",
    "'''\n",
    "Now we will create teh final list of bigrams..they should have the following property\n",
    "1. The frequency of the bigram should be more than 20\n",
    "2. both the parts should be unigrams individually\n",
    "3. both the parts should not be same 'temp temp' is not valid\n",
    "'''\n",
    "finalBigramFeatures=[]\n",
    "for key in bigramCounts:\n",
    "    if(bigramCounts[key]>20):\n",
    "        #print(key)\n",
    "        arrBigrams=key.split()\n",
    "        if(arrBigrams[0]!=arrBigrams[1]):\n",
    "            #iff the individual words are part of unigrams then only add it\n",
    "            if((arrBigrams[0] in unigrams) and (arrBigrams[1] in unigrams)):\n",
    "                finalBigramFeatures.append(key)\n",
    "\n",
    "#finalBigramFeatures\n",
    "print(len(finalBigramFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### similarly for the trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11901\n",
      "60634\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "'''Get the dictioanries for individual cases'''\n",
    "trigramFeatures=[]\n",
    "for case in cleanedUpCases:\n",
    "    trigramFeatures.append(get_trigrams(case))\n",
    "print(len(trigramFeatures))\n",
    "\n",
    "'''Combine the dictionaries to get the counts of individual trigrams'''\n",
    "trigramCounts={}\n",
    "for trigramFeature in trigramFeatures:\n",
    "    for k, v in trigramFeature:\n",
    "        key=(' '.join(k))\n",
    "        if key not in trigramCounts.keys():\n",
    "            trigramCounts[key]=v\n",
    "        else:\n",
    "            trigramCounts[key]+=v\n",
    "\n",
    "print(len(trigramCounts))\n",
    "\n",
    "\n",
    "'''\n",
    "Now we will create teh final list of trigrams..they should have the following property\n",
    "1. The frequency of the trigrams should be more than 20\n",
    "2. all the parts should be unigrams individually\n",
    "3. all the parts should not be same 'temp temp temp' is not valid\n",
    "'''\n",
    "finalTrigramFeatures=[]\n",
    "for key in trigramCounts:\n",
    "    if(trigramCounts[key]>20):\n",
    "        #print(key)\n",
    "        arrTrigrams=key.split()\n",
    "        if(arrTrigrams[0]!=arrTrigrams[1]!=arrTrigrams[2]):\n",
    "            #iff the individual words are part of unigrams then only add it\n",
    "            if((arrTrigrams[0] in unigrams) and (arrTrigrams[1] in unigrams) and (arrTrigrams[2] in unigrams)):\n",
    "                finalTrigramFeatures.append(key)\n",
    "\n",
    "#finalBigramFeatures\n",
    "print(len(finalTrigramFeatures))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the spacy code for the bigrams and trigrams here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Utility function to cleanup the nounphrases'''\n",
    "\n",
    "def cleanupNounPhrase(str):\n",
    "    str = str.strip();\n",
    "    str = re.sub('/[^A-Za-z0-9 _\\-\\+\\&\\,\\#]/', '', str)\n",
    "    str = str.replace('\"', ' ')\n",
    "    str = str.replace('\\\"', ' ')\n",
    "    str = str.replace(')', ' ')\n",
    "    str = str.replace('(', ' ')\n",
    "    str = str.replace('>', ' ')\n",
    "    str = str.replace('@', ' ')\n",
    "    str = str.replace('<', ' ')\n",
    "    str = str.replace(':', ' ')\n",
    "    str = str.replace('.', ' ')\n",
    "    str = str.replace('[', ' ')\n",
    "    str = str.replace(']', ' ')\n",
    "    str = str.replace('_', ' ')\n",
    "    str = str.replace(',', ' ')\n",
    "    str = str.replace('#', ' ')\n",
    "    str = str.replace('-', ' ')\n",
    "    str = str.replace('/', ' ')\n",
    "    str = str.replace('\"', ' ')\n",
    "    str = str.replace('\\n', ' ')\n",
    "    str = str.replace('~', ' ')\n",
    "    str = re.sub(r'\\d+', ' ', str)\n",
    "    word_sent = [word for word in str.lower().split(\" \") if word not in stopwordList and len(word)>1]\n",
    "    if (len(word_sent) > 1):\n",
    "        finalSent = ' '.join(word_sent)\n",
    "    else:\n",
    "        finalSent=''\n",
    "\n",
    "    return(finalSent.strip())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11901\n",
      "after flatteing noun phrases 22785\n",
      "8182\n",
      "303\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from spacy.en import English\n",
    "nlp = English()\n",
    "\n",
    "docs = [nlp(d) for d in cases]\n",
    "\n",
    "'''Get the noun phrases'''\n",
    "arrNounPhrase=[]\n",
    "for idx, doc in enumerate(docs):\n",
    "    testDoc=doc\n",
    "    token_nounPhrases = [np.text for np in testDoc.noun_chunks]\n",
    "    updatedNounPhrases=[]\n",
    "    for tt in token_nounPhrases:\n",
    "        finalSent=cleanupNounPhrase(tt)\n",
    "        if(len(finalSent)>0):\n",
    "            updatedNounPhrases.append(finalSent)\n",
    "    arrNounPhrase.append(updatedNounPhrases)\n",
    "print(len(arrNounPhrase))\n",
    "\n",
    "'''\n",
    "Flattening the arrNounPhrase\n",
    "'''\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "flatNounPhraseList=flatten(arrNounPhrase)\n",
    "print(\"after flatteing noun phrases\",len(flatNounPhraseList))\n",
    "\n",
    "'''Now we need to tabulate the results'''\n",
    "nounPhraseDictCount={}\n",
    "import re\n",
    "#now we can apply the cleanup\n",
    "for tt in flatNounPhraseList:\n",
    "    finalSent=tt\n",
    "    if (nounPhraseDictCount.get(finalSent)):\n",
    "        nounPhraseDictCount[finalSent] += 1\n",
    "    else:\n",
    "        nounPhraseDictCount[finalSent] = 1\n",
    "\n",
    "print(len(nounPhraseDictCount))\n",
    "\n",
    "'''out of all the nounphrases pick up the once which are more frequncet'''\n",
    "finalPOSList={}\n",
    "for dd in nounPhraseDictCount:\n",
    "    ddArr=dd.split(\" \")\n",
    "    if(len(ddArr)>1):\n",
    "        #atelaset 10 occurence of the phrase\n",
    "        if(nounPhraseDictCount[dd]>9):\n",
    "            #print(dd,nounPhraseDictCount[dd])\n",
    "            finalPOSList[dd]=nounPhraseDictCount[dd]\n",
    "            #finalPOSList.append()\n",
    "\n",
    "print(len(finalPOSList))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "['elevated vibrations', 'pressure steam tag', 'nde seal outlet flow', 'cylinder exhaust temperature', 'seal oil pressure', 'rr notification', 'vibration issues', 'turbine outlet temp', 'inlet position', 'agb chip detector', 'driver thrust bearing', 'gen slipring bearing vib', 'gt vibration transducer', 'lube oil filter dp', 'rotor vib brg', 'casing vibrations', 'mechanical model', 'de seal outlet flow', 'exhaust duct pressure', 'rear gen bearing temp', 'observed temperature', 'power turbine positions', 'nde vibration', 'air compressor', 'pump ib brg temp', 'driver load', 'turbine exhaust pressure', 'water jacket inlet pressure', 'probe sample', 'intercooler coolant pressure', 'wheelspace differential', 'generator de bearing temperature', 'compressor outlet temp range', 'fw pmp axial probe', 'gearbox oil temperature', 'current conditions', 'motor winding temperature', 'rotor speed', 'compressor speed', 'motor ob bearing temp', 'axial positions', 'intercooler outlet temperature', 'thrust bearing axial position', 'hydrogen pressure', 'combustion monitor spread', 'journal bearings', 'gt load', 'motor temp', 'bowl dp', 'motor stator temps', 'drive generator bearing temp', 'generator bearing metal temp', 'inboard vibration', 'fuel temperature', 'stator coil vib', 'hrsg temp', 'residual thresholds', 'de seal dp', 'drive generator bearing temperature', 'axial displacement', 'wind turbines', 'bearing oil drain temperature', 'exhaust steam temperature', 'lp vib', 'chip detectors', 'exhaust temp spread', 'hydrogen gas pressure', 'mech model', 'temperature issues', 'duct firing', 'low alarm', 'engine speed', 'ip hrsg drum sample', 'seal gas filter', 'rear gear bearing temp', 'generator bearing temperature de', 'fuel gas temperature', 'de bearing vibrations', 'sh dsh outlet temperature', 'mtl temps', 'attemperator spray flow', 'gt nox', 'generator bearing temperature nde', 'thermal mechanical tag', 'current reading']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "need to further check 2 things \n",
    "1. Each of the individual word is a unigrams\n",
    "2. The nounphrase is not already present in the bigrams or trigrams \n",
    "'''\n",
    "\n",
    "listPOSNounPhrases=[]\n",
    "for k in finalPOSList:\n",
    "    listPOSNounPhrases.append(k)\n",
    "\n",
    "finallyCheckedNounPhrases=[] \n",
    "finalQuadgramFeatures=[]\n",
    "for key in listPOSNounPhrases:\n",
    "    arrNgrams=key.split()\n",
    "    flag=True\n",
    "    length=len(arrNgrams)\n",
    "    #print(key)\n",
    "    for i in range(length):\n",
    "        #print(i)\n",
    "        if(arrNgrams[i] not in unigrams):\n",
    "            flag=False\n",
    "                \n",
    "    if (flag==True):\n",
    "        if(length==2):\n",
    "            #check the presence in bigrams\n",
    "            if(key not in finalBigramFeatures):\n",
    "                finallyCheckedNounPhrases.append(key)\n",
    "                finalBigramFeatures.append(key)\n",
    "        if(length==3):\n",
    "            #check the presence in trigrams\n",
    "            if(key not in finalTrigramFeatures):\n",
    "                finallyCheckedNounPhrases.append(key)\n",
    "                finalTrigramFeatures.append(key)\n",
    "        if(length==4):\n",
    "            finallyCheckedNounPhrases.append(key)\n",
    "            finalQuadgramFeatures.append(key)\n",
    "\n",
    "print(len(finallyCheckedNounPhrases))\n",
    "print(finallyCheckedNounPhrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram length-> 605\n",
      "bigrams length-> 769\n",
      "trigrams length-> 252\n",
      "quadgrams length-> 21\n"
     ]
    }
   ],
   "source": [
    "'''Following is the length of variosu ngrams'''\n",
    "print(\"unigram length->\",len(unigrams))\n",
    "print(\"bigrams length->\",len(finalBigramFeatures))\n",
    "print(\"trigrams length->\",len(finalTrigramFeatures))\n",
    "print(\"quadgrams length->\",len(finalQuadgramFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now have to normalize the ngrams using their stemmed version\n",
    "\n",
    "The steps are as follows\n",
    "1. First create a map of stem and all the words that match this stem \n",
    "e.g. 'increas': ('increase', 895, 'increasing', 676, 'increases', 313)\n",
    "\n",
    "2. Create a word to stem mapping. \n",
    "e.g increase: increas\n",
    "\n",
    "3. Create a map of final stem to word map that is all the words that match this stem should be replaced by this word e.g.\n",
    "\n",
    "increas : increase\n",
    "\n",
    "decreas: decreased\n",
    "\n",
    "    This will use the map that was created in step 1\n",
    "\n",
    "4. now use the maps in step 2 and 3 to find the final dictionary\n",
    "\n",
    "word1->stem\n",
    "stem->normalizedword\n",
    "\n",
    "will give\n",
    "\n",
    "word1->nomralized word\n",
    "\n",
    "increasing : increase\n",
    "increases : increase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479\n",
      "479\n",
      "605\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "''' \n",
    "Create a dictioanry of stem-> all words\n",
    "{ 'decreas': ('decreased', 1149, 'decrease', 290, 'decreasing', 203, 'decreases', 91),...}\n",
    "'''\n",
    "stemmingDictBasedOnFreq={}\n",
    "for k,v in sortedFreqDict:\n",
    "    stemKey=stemmer.stem(k)\n",
    "    if (stemKey not in stemmingDictBasedOnFreq):\n",
    "        stemmingDictBasedOnFreq[stemKey]=(k,v)\n",
    "    else:\n",
    "        stemmingDictBasedOnFreq[stemKey]+=(k,v)\n",
    "\n",
    "print(len(stemmingDictBasedOnFreq))\n",
    "\n",
    "'''\n",
    "Create teh stem to word mapping. For all the stems that were found earlier we need to find which word will replace it \n",
    "THis word will be selected from the list of words that map to the same stem\n",
    "increas : increase\n",
    "\n",
    "'''\n",
    "stemmingDictFinal={}\n",
    "for k in stemmingDictBasedOnFreq:\n",
    "    #print(k)\n",
    "    #take the fisrst entry from the list that is stored for each stem value\n",
    "    stemmingDictFinal[k]=list(stemmingDictBasedOnFreq[k])[0]\n",
    "\n",
    "print(len(stemmingDictFinal))\n",
    "\n",
    "'''\n",
    "Create a word to stem mapping. For each unigram it will be created\n",
    "'''\n",
    "#word->stem mapping\n",
    "wordToStemMapping={}\n",
    "for k in unigrams:\n",
    "    #get the stem of the word\n",
    "    stemKey=stemmer.stem(k)\n",
    "    #create a map of word to its stem\n",
    "    wordToStemMapping[k]=stemKey\n",
    "\n",
    "print(len(wordToStemMapping))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now get the normalized version of each unigram using the stems map\n",
    "We will also make use of the somain information to cheange some of the unigrams like rot to rotor that cannot be done using the stem mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605\n",
      "pump\n"
     ]
    }
   ],
   "source": [
    "normalizedUnigramsDict={}\n",
    "for k in wordToStemMapping:\n",
    "    stem=wordToStemMapping[k]\n",
    "   #print(k,stem,stemmingDictFinal[stem])\n",
    "    normalizedUnigramsDict[k]=stemmingDictFinal[stem]\n",
    "\n",
    "print(len(normalizedUnigramsDict))\n",
    "\n",
    "#bring in the domain information as well\n",
    "for k in normalizedUnigramsDict:\n",
    "    for j in domainDict:\n",
    "        if (k==j):\n",
    "           # print(k,j,domainDict[j])\n",
    "            normalizedUnigramsDict[k]=domainDict[j]\n",
    "\n",
    "print(normalizedUnigramsDict['pmp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize bigrams and trigrams\n",
    "For the bigrams and trigrams it is basically normalizing the individual parts of the ngram\n",
    "Create a generic function for the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Takes the following\n",
    "ngramListToNormalize: The bigram or trigram input list to normalize\n",
    "normalizedUnigramDict: The unigram normalized dictionary containing raw->normalized unigram\n",
    "N: ngrams n\n",
    "'''\n",
    "def createNormalizedWordDict(ngramListToNormalize,normalizedUnigramDict,N):\n",
    "    normalizedNgramsDict={}\n",
    "    for k in ngramListToNormalize:\n",
    "        arrgrams=k.split()\n",
    "        str=''\n",
    "        for i in range(N):\n",
    "            str+=' '+normalizedUnigramDict[arrgrams[i]]\n",
    "        normalizedNgramsDict[k]=str.strip()\n",
    "    return normalizedNgramsDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769 769\n",
      "252 252\n",
      "21 21\n"
     ]
    }
   ],
   "source": [
    "normalizedBigramsDict=createNormalizedWordDict(finalBigramFeatures,normalizedUnigramsDict,2)\n",
    "normalizedTrigramsDict=createNormalizedWordDict(finalTrigramFeatures,normalizedUnigramsDict,3)\n",
    "normalizedQuadgramsDict=createNormalizedWordDict(finalQuadgramFeatures,normalizedUnigramsDict,4)\n",
    "\n",
    "print(len(finalBigramFeatures), len(normalizedBigramsDict))\n",
    "print(len(finalTrigramFeatures), len(normalizedTrigramsDict))\n",
    "print(len(finalQuadgramFeatures), len(normalizedQuadgramsDict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the dictioaries together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1686\n"
     ]
    }
   ],
   "source": [
    "finalNgramDict=normalizedUnigramsDict.copy()\n",
    "finalNgramDict.update(normalizedBigramsDict)\n",
    "finalNgramDict.update(normalizedTrigramsDict)\n",
    "finalNgramDict.update(normalizedQuadgramsDict)\n",
    "#add the domain words also\n",
    "finalNgramDict.update(domainDict)\n",
    "\n",
    "print(len(finalNgramDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(domainDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump the dictionary to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option1: pickle the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "output = open('outputDict.txt', 'ab+')\n",
    "pickle.dump(finalNgramDict, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option2: write to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('dict.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for key in finalNgramDict:\n",
    "        writer.writerow([key,finalNgramDict[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for k,v in finalNgramDict.items():\n",
    "#     if (k==\"metal temp reading\"):\n",
    "#         print(k,v)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
